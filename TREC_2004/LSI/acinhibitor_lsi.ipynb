{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ACINHIBITOR RUNS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the models in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CR107\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required Libraries loaded.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from time import time\n",
    "import os\n",
    "import logging\n",
    "import scipy\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import gensim\n",
    "import re\n",
    "from gensim import models, corpora, similarities\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from sklearn import metrics, preprocessing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
    "from scipy.stats.distributions import randint\n",
    "from sklearn.cross_validation import StratifiedKFold, train_test_split, cross_val_score\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.cross_validation import StratifiedKFold, KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier, Perceptron, PassiveAggressiveClassifier\n",
    "\n",
    "#\n",
    "\n",
    "print (\"Required Libraries loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import file in cvs format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-16 12:44:22,167 : INFO : loading Dictionary object from C:\\Users\\CR107\\Dropbox\\PhD\\Experiments\\TREC_2004\\LSI/dict/aceinhibitor.dict\n",
      "2017-01-16 12:44:22,262 : INFO : loaded corpus index from C:\\Users\\CR107\\Dropbox\\PhD\\Experiments\\TREC_2004\\LSI/dict/aceinhibitor.mm.index\n",
      "2017-01-16 12:44:22,263 : INFO : initializing corpus reader from C:\\Users\\CR107\\Dropbox\\PhD\\Experiments\\TREC_2004\\LSI/dict/aceinhibitor.mm\n",
      "2017-01-16 12:44:22,323 : INFO : accepted corpus with 2498 documents, 10730 features, 259641 non-zero entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Needed files loaded from disk\n",
      "MmCorpus(2498 documents, 10730 features, 259641 non-zero entries)\n"
     ]
    }
   ],
   "source": [
    "#load stored corpus from disk\n",
    "dict_path = \"C:\\Users\\CR107\\Dropbox\\PhD\\Experiments\\TREC_2004\\LSI/dict/aceinhibitor.dict\"\n",
    "mm_path = \"C:\\Users\\CR107\\Dropbox\\PhD\\Experiments\\TREC_2004\\LSI/dict/aceinhibitor.mm\"\n",
    "if (os.path.exists(dict_path)):\n",
    "    dictionary = corpora.Dictionary.load(dict_path)\n",
    "    corpus = corpora.MmCorpus(mm_path)\n",
    "    print(\"Needed files loaded from disk\")\n",
    "else:\n",
    "    print(\"Please run the data_prep.ipynb file to generate data set\")\n",
    "\n",
    "print (corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-16 16:15:27,769 : INFO : collecting document frequencies\n",
      "2017-01-16 16:15:27,770 : INFO : PROGRESS: processing document #0\n",
      "2017-01-16 16:15:28,678 : INFO : calculating IDF weights for 2498 documents and 10729 features (259641 matrix non-zeros)\n",
      "2017-01-16 16:15:28,686 : INFO : using serial LSI version on this node\n",
      "2017-01-16 16:15:28,687 : INFO : updating model with new documents\n",
      "2017-01-16 16:15:29,815 : INFO : preparing a new chunk of documents\n",
      "2017-01-16 16:15:29,858 : INFO : using 100 extra samples and 2 power iterations\n",
      "2017-01-16 16:15:29,858 : INFO : 1st phase: constructing (10730L, 2100L) action matrix\n",
      "2017-01-16 16:15:30,591 : INFO : orthonormalizing (10730L, 2100L) action matrix\n",
      "2017-01-16 16:15:40,273 : INFO : 2nd phase: running dense svd on (2100L, 2498L) matrix\n",
      "2017-01-16 16:15:45,832 : INFO : computing the final decomposition\n",
      "2017-01-16 16:15:45,834 : INFO : keeping 2000 factors (discarding 0.819% of energy spectrum)\n",
      "2017-01-16 16:15:46,799 : INFO : processed documents up to #2498\n",
      "2017-01-16 16:15:46,802 : INFO : topic #0(8.850): -0.140*\"enalapril\" + -0.122*\"captopril\" + -0.120*\"mg\" + -0.118*\"failure\" + -0.118*\"heart\" + -0.113*\"bp\" + -0.111*\"p\" + -0.110*\"lisinopril\" + -0.110*\"group\" + -0.110*\"losartan\"\n",
      "2017-01-16 16:15:46,803 : INFO : topic #1(4.832): 0.228*\"bp\" + -0.215*\"failure\" + -0.202*\"myocardial\" + 0.189*\"hg\" + -0.189*\"heart\" + -0.184*\"ventricular\" + 0.183*\"mm\" + -0.161*\"infarction\" + 0.153*\"mg\" + -0.133*\"left\"\n",
      "2017-01-16 16:15:46,805 : INFO : topic #2(4.296): 0.284*\"renal\" + 0.266*\"diabetic\" + 0.229*\"diabetes\" + -0.202*\"ventricular\" + 0.194*\"type\" + -0.173*\"left\" + -0.170*\"lv\" + 0.141*\"nephropathy\" + 0.129*\"proteinuria\" + 0.127*\"mellitus\"\n",
      "2017-01-16 16:15:46,806 : INFO : topic #3(4.086): 0.480*\"losartan\" + 0.169*\"receptor\" + 0.164*\"cough\" + 0.162*\"angiotensin\" + 0.142*\"failure\" + -0.140*\"lv\" + -0.137*\"diabetic\" + 0.131*\"ii\" + -0.130*\"ramipril\" + -0.120*\"left\"\n",
      "2017-01-16 16:15:46,808 : INFO : topic #4(3.860): -0.278*\"losartan\" + 0.168*\"cardiovascular\" + -0.163*\"captopril\" + 0.150*\"risk\" + -0.134*\"enalapril\" + 0.133*\"trials\" + -0.126*\"plasma\" + -0.125*\"exercise\" + -0.118*\"renal\" + 0.117*\"hg\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "tfidf = models.TfidfModel(corpus) # step 1 -- initialize a model\n",
    "\n",
    "corpus_tfidf = tfidf[corpus] ##apply the transformation to all corpus\n",
    "#for doc in corpus_tfidf:\n",
    "#    print(doc)\n",
    "\n",
    "\n",
    "lsi_model = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2000) # initialize an LSI transformation\n",
    "corpus_lsi = lsi_model[corpus_tfidf] # create a double wrapper over the original corpus: bow->tfidf->fold-in-lsi\n",
    "\n",
    "print (\"Done.\")\n",
    "\n",
    "#lsi_model.save('/tmp/model.lsi') # same for tfidf, lda, ...\n",
    "#lsi_model = models.LsiModel.load('/tmp/model.lsi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-16 13:47:47,040 : INFO : computing word-topic salience for 5 topics\n",
      "2017-01-16 13:47:47,210 : INFO : topic #0(8.850): patients(0.076), study(0.062), middle(0.027), treatment(0.072), aged(0.043), ..., \n",
      "2017-01-16 13:47:47,223 : INFO : topic #1(4.832): heart(-0.189), failure(-0.215), myocardial(-0.202), ventricular(-0.184), infarction(-0.161), ..., hg(0.189), mm(0.183)\n",
      "2017-01-16 13:47:47,240 : INFO : topic #2(4.296): renal(-0.284), diabetic(-0.266), diabetes(-0.229), mellitus(-0.127), type(-0.194), ..., ventricular(0.202), left(0.173)\n",
      "2017-01-16 13:47:47,256 : INFO : topic #3(4.086): losartan(0.480), receptor(0.169), angiotensin(0.162), angiotensinantagonists(0.081), ii(0.131), ..., left(-0.120), ventricular(-0.119)\n",
      "2017-01-16 13:47:47,272 : INFO : topic #4(3.860): losartan(-0.277), plasma(-0.126), captopril(-0.163), measured(-0.038), enalapril(-0.134), ..., cardiovascular(0.168), risk(0.150)\n",
      "2017-01-16 13:47:47,279 : INFO : topic #0(8.850): 0.140*\"enalapril\" + 0.122*\"captopril\" + 0.120*\"mg\" + 0.118*\"failure\" + 0.118*\"heart\"\n",
      "2017-01-16 13:47:47,282 : INFO : topic #1(4.832): 0.228*\"bp\" + -0.215*\"failure\" + -0.202*\"myocardial\" + 0.189*\"hg\" + -0.189*\"heart\"\n",
      "2017-01-16 13:47:47,285 : INFO : topic #2(4.296): -0.284*\"renal\" + -0.266*\"diabetic\" + -0.229*\"diabetes\" + 0.202*\"ventricular\" + -0.194*\"type\"\n",
      "2017-01-16 13:47:47,286 : INFO : topic #3(4.086): 0.480*\"losartan\" + 0.169*\"receptor\" + 0.164*\"cough\" + 0.162*\"angiotensin\" + 0.142*\"failure\"\n",
      "2017-01-16 13:47:47,289 : INFO : topic #4(3.860): -0.277*\"losartan\" + 0.168*\"cardiovascular\" + -0.163*\"captopril\" + 0.150*\"risk\" + -0.134*\"enalapril\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  u'0.140*\"enalapril\" + 0.122*\"captopril\" + 0.120*\"mg\" + 0.118*\"failure\" + 0.118*\"heart\"'),\n",
       " (1,\n",
       "  u'0.228*\"bp\" + -0.215*\"failure\" + -0.202*\"myocardial\" + 0.189*\"hg\" + -0.189*\"heart\"'),\n",
       " (2,\n",
       "  u'-0.284*\"renal\" + -0.266*\"diabetic\" + -0.229*\"diabetes\" + 0.202*\"ventricular\" + -0.194*\"type\"'),\n",
       " (3,\n",
       "  u'0.480*\"losartan\" + 0.169*\"receptor\" + 0.164*\"cough\" + 0.162*\"angiotensin\" + 0.142*\"failure\"'),\n",
       " (4,\n",
       "  u'-0.277*\"losartan\" + 0.168*\"cardiovascular\" + -0.163*\"captopril\" + 0.150*\"risk\" + -0.134*\"enalapril\"')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi_model.print_debug(num_topics=5, num_words=5)\n",
    "lsi_model.print_topic(topicno=5, topn=5)\n",
    "lsi_model.print_topics(num_topics=5, num_words=5)\n",
    "#for doc in corpus_lsi: # both bow->tfidf and tfidf->lsi transformations are actually executed here, on the fly\n",
    "#     print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-01-16 16:09:59,809 : INFO : using symmetric alpha at 0.0005\n",
      "2017-01-16 16:09:59,813 : INFO : using symmetric eta at 0.0005\n",
      "2017-01-16 16:09:59,818 : INFO : using serial LDA version on this node\n",
      "2017-01-16 16:13:42,131 : INFO : running online LDA training, 2000 topics, 1 passes over the supplied corpus of 2498 documents, updating model once every 2000 documents, evaluating perplexity every 2498 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2017-01-16 16:13:42,134 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      "2017-01-16 16:13:43,006 : INFO : PROGRESS: pass 0, at document #2000/2498\n",
      "2017-01-16 16:14:43,947 : INFO : merging changes from 2000 documents into a model of 2498 documents\n",
      "2017-01-16 16:14:46,496 : INFO : topic #1869 (0.001): 0.001*pregnancy + 0.001*perinatal + 0.001*nephropathiesdiagnosisdrug + 0.001*adequate + 0.001*reproductive + 0.001*renal + 0.001*fetal + 0.001*albuminuria + 0.001*pregnant + 0.000*maternal\n",
      "2017-01-16 16:14:46,562 : INFO : topic #1529 (0.001): 0.033*telmisartan + 0.028*vitamin + 0.015*percent + 0.015*e + 0.014*exercise + 0.012*cardiovascular + 0.011*enalapril + 0.010*risk + 0.009*continuation + 0.009*assigned\n",
      "2017-01-16 16:14:46,628 : INFO : topic #163 (0.001): 0.000*intravaginal + 0.000*physically + 0.000*iiphysiology + 0.000*induceddiagnosisphysiopathologytherapy + 0.000*pre + 0.000*tissuespecific + 0.000*resuscitation + 0.000*raises + 0.000*endometrial + 0.000*postdialysis\n",
      "2017-01-16 16:14:46,694 : INFO : topic #1209 (0.001): 0.033*irbesartan + 0.026*children + 0.022*ii + 0.018*angiotensin + 0.016*kidney + 0.015*antagonists + 0.015*inhibiting + 0.015*options + 0.013*proteinuria + 0.012*reninangiotensinaldosterone\n",
      "2017-01-16 16:14:46,759 : INFO : topic #1953 (0.001): 0.030*stophypertension + 0.022*swedish + 0.018*antiarrhythmic + 0.015*complexes + 0.014*congestive + 0.010*tachycardia + 0.010*heart + 0.010*old + 0.010*arrhythmias + 0.009*digoxin\n",
      "2017-01-16 16:14:46,851 : INFO : topic diff=1969.504244, rho=1.000000\n",
      "2017-01-16 16:15:07,336 : INFO : -162.721 per-word bound, 9633035328207248823565356820944916305665918500864.0 perplexity estimate based on a held-out corpus of 498 documents with 3416 words\n",
      "2017-01-16 16:15:07,338 : INFO : PROGRESS: pass 0, at document #2498/2498\n",
      "2017-01-16 16:15:23,042 : INFO : merging changes from 498 documents into a model of 2498 documents\n",
      "2017-01-16 16:15:25,648 : INFO : topic #600 (0.001): 0.026*revascularizationmethods + 0.005*aggressive + 0.005*revascularization + 0.004*goals + 0.004*art + 0.004*courage + 0.003*coronary + 0.003*pci + 0.003*percutaneous + 0.003*management\n",
      "2017-01-16 16:15:25,713 : INFO : topic #707 (0.001): 0.094*inducedpathologytherapy + 0.023*suggested + 0.008*arf + 0.007*tubular + 0.007*necrosis + 0.006*renal + 0.004*baselaged + 0.004*confirmed + 0.004*immunoallergic + 0.004*biopsy\n",
      "2017-01-16 16:15:25,779 : INFO : topic #1368 (0.001): 0.000*intravaginal + 0.000*physically + 0.000*iiphysiology + 0.000*induceddiagnosisphysiopathologytherapy + 0.000*pre + 0.000*tissuespecific + 0.000*resuscitation + 0.000*raises + 0.000*endometrial + 0.000*postdialysis\n",
      "2017-01-16 16:15:25,841 : INFO : topic #1989 (0.001): 0.063*channel + 0.050*calcium + 0.041*grown + 0.031*pulsedrug + 0.027*effectsmh + 0.026*collagen + 0.025*benidipine + 0.024*therapyphysiopathologymh + 0.020*delapril + 0.018*mainly\n",
      "2017-01-16 16:15:25,907 : INFO : topic #1091 (0.001): 0.003*cilexetil + 0.003*candesartan + 0.002*amlodipine + 0.001*edema + 0.001*amlodipineadverse + 0.001*effectstherapeutic + 0.001*mild + 0.001*bp + 0.001*tolerability + 0.001*peripheral\n",
      "2017-01-16 16:15:25,999 : INFO : topic diff=inf, rho=0.707107\n"
     ]
    }
   ],
   "source": [
    "lda_model = models.LdaModel(corpus_tfidf, id2word=dictionary, num_topics=2000)\n",
    "#lda_model.print_topics(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scipy_csc_matrix = gensim.matutils.corpus2csc(corpus_lsi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2498, 500)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy_matrix = scipy_csc_matrix.T\n",
    "scipy_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import the data and explore the first few rows\n",
    "# Import the data and explore the first few rows\n",
    "\n",
    "inhibitor  = pd.read_csv(\"C:\\EPC_Data\\TREC_BROKEN\\No_Mh_Tag/aceinhibitor_no_mh.csv\", sep=\",\")#, index_col = \"PMID\")\n",
    "seed = 55\n",
    "# shuffle dataset and split to train and test\n",
    "x, scipy_matrix = shuffle(inhibitor, scipy_matrix, random_state = seed)\n",
    "y_CV = x.Label\n",
    "\n",
    "#split to test and train\n",
    "X_train, X_test, y_train, y_test = train_test_split(scipy_matrix, x.Label, random_state = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fit svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 1.000000\n",
      "Test set score: 0.984000\n",
      "[[611   7]\n",
      " [  3   4]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.99      0.99       618\n",
      "          1       0.36      0.57      0.44         7\n",
      "\n",
      "avg / total       0.99      0.98      0.99       625\n",
      "\n",
      "Overall Accuracy: 0.98\n",
      "Number of support vectors for each class:  [93 27]\n"
     ]
    }
   ],
   "source": [
    "##fit SVM\n",
    "svm = SVC(C=1000.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
    "    decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
    "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "    tol=0.001, verbose=False)\n",
    "svm.fit(X_train, y_train)\n",
    "svmpred = svm.predict(X_test)\n",
    "\n",
    "print(\"Training set score: %f\" % svm.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % svm.score(X_test, y_test))\n",
    "\n",
    "# Get the confusion matrix for your classifier using metrics.confusion_matrix\n",
    "mat = metrics.confusion_matrix(y_test, svmpred)    \n",
    "print(mat)\n",
    "\n",
    "\n",
    "print(metrics.classification_report(y_test, svmpred))\n",
    "print(\"Overall Accuracy:\", round(metrics.accuracy_score(y_test, svmpred),2))\n",
    "\n",
    "\n",
    "# get number of support vectors for each class \n",
    "print(\"Number of support vectors for each class: \", svm.n_support_)\n",
    "\n",
    "#Indices of support vectors\n",
    "#print(\"Support vector indices: \", svm.support_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of support vectors for each class:  [1265   27]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adjust with sample weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    x = np.where(x==0, 1, 4)\n",
    "    return x \n",
    "\n",
    "sw = func(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.959423\n",
      "Test set score: 0.948800\n",
      "[[588  30]\n",
      " [  2   5]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.95      0.97       618\n",
      "          1       0.14      0.71      0.24         7\n",
      "\n",
      "avg / total       0.99      0.95      0.97       625\n",
      "\n",
      "Overall Accuracy: 0.95\n",
      "Number of support vectors for each class:  [623  23]\n"
     ]
    }
   ],
   "source": [
    "#fit with weight\n",
    "svm = SVC(C=100.0, cache_size=200, class_weight='balanced', coef0=0.0,\n",
    "    decision_function_shape=None, degree=3, gamma='auto', kernel='sigmoid',\n",
    "    max_iter=-1, probability=False, random_state=seed, shrinking=True,\n",
    "    tol=0.001, verbose=False)\n",
    "\n",
    "\n",
    "\n",
    "svm.fit(X_train, y_train, sample_weight = sw)\n",
    "svmpred = svm.predict(X_test)\n",
    "\n",
    "print(\"Training set score: %f\" % svm.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % svm.score(X_test, y_test))\n",
    "\n",
    "# Get the confusion matrix for your classifier using metrics.confusion_matrix\n",
    "mat = metrics.confusion_matrix(y_test, svmpred)    \n",
    "print(mat)\n",
    "\n",
    "\n",
    "print(metrics.classification_report(y_test, svmpred))\n",
    "print(\"Overall Accuracy:\", round(metrics.accuracy_score(y_test, svmpred),2))\n",
    "\n",
    "\n",
    "##get support vector details\n",
    "#print (\"Support Vectors: \", svm.support_vectors_)\n",
    "\n",
    "# get number of support vectors for each class \n",
    "print(\"Number of support vectors for each class: \", svm.n_support_)\n",
    "\n",
    "#Indices of support vectors\n",
    "#print(\"Support vector indices: \", svm.support_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SGD Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set score: 0.961025\n",
      "Test set score: 0.939200\n",
      "[[580  38]\n",
      " [  0   7]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.94      0.97       618\n",
      "          1       0.16      1.00      0.27         7\n",
      "\n",
      "avg / total       0.99      0.94      0.96       625\n",
      "\n",
      "Overall Accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "\n",
    "p_clf = Perceptron(penalty = 'l2',alpha = 0.001,fit_intercept = True, n_iter = 25, shuffle = True, verbose = 0, \n",
    "                   eta0 = 0.1, n_jobs = 1, random_state = seed, class_weight = 'balanced', warm_start = False)\n",
    "\n",
    "\n",
    "p_clf.fit(X_train, y_train, sample_weight = sw)\n",
    "\n",
    "\n",
    "\n",
    "p_pred = p_clf.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"Training set score: %f\" % p_clf.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % p_clf.score(X_test, y_test))\n",
    "\n",
    "mat = metrics.confusion_matrix(y_test, p_pred)    \n",
    "print(mat)\n",
    "\n",
    "sw_test = func(y_test)\n",
    "print(metrics.classification_report(y_test, p_pred))\n",
    "print(\"Overall Accuracy:\", round(metrics.accuracy_score(y_test, p_pred, sample_weight = sw_test),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### use the perceptron for cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recalls:  [ 0.625  0.625]\n",
      "Average Recall:  0.625\n",
      "Precisions: [ 0.12820513  0.12820513]\n",
      "Average Precision: 0.128205128205\n"
     ]
    }
   ],
   "source": [
    "#10-fold CV\n",
    "cv_p_clf = Perceptron(penalty = 'l2',alpha = 0.001,fit_intercept = True, n_iter = 25, shuffle = True, verbose = 0, \n",
    "                   eta0 = 0.1, n_jobs = 1, random_state = 1, class_weight = 'balanced', warm_start = False)\n",
    "\n",
    "\n",
    "scores = cross_val_score(cv_p_clf, X_train, y_train, cv = 2, scoring = 'recall', fit_params = {'sample_weight':sw})\n",
    "prec_scores = cross_val_score(cv_p_clf, X_train, y_train, cv = 2, scoring = 'precision', fit_params = {'sample_weight':sw})\n",
    "\n",
    "print (\"Recalls: \", scores)\n",
    "print (\"Average Recall: \", scores.mean())\n",
    "\n",
    "print (\"Precisions:\", prec_scores)\n",
    "print (\"Average Precision:\", prec_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done transforming\n"
     ]
    }
   ],
   "source": [
    "X_CV = scipy_matrix\n",
    "print (\"Done transforming\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "Fitting the model...\n",
      "Perceptron Cross validation results: \n",
      "Precision: 0.02 \n",
      "   Recall: 0.08 \n",
      "       F1: 0.03 \n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#np.random.seed(55)\n",
    "## use stratified 5X2-fold CV\n",
    "# first merge and preprocess data\n",
    "print (\"starting\")\n",
    "\n",
    "\n",
    "p_precision = []\n",
    "p_recall = []\n",
    "p_f1 = []\n",
    "print (\"Fitting the model...\")\n",
    "\n",
    "cv_p_clf = Perceptron(penalty = 'l2',alpha = 0.001,fit_intercept = True, n_iter = 10, shuffle = True, verbose = 0, \n",
    "                   eta0 = 0.1, n_jobs = 1, random_state = seed, class_weight = 'balanced', warm_start = False)\n",
    "\n",
    "for i in range(5):\n",
    "    skf = StratifiedKFold(y_CV, n_folds=2, shuffle = True, random_state = seed)\n",
    "    for train_index, test_index in skf:\n",
    "        \n",
    "        CV_train, CV_test = X_CV[train_index], X_CV[test_index]\n",
    "        y_CVtrain, y_CVtest = y_CV[train_index], y_CV[test_index]\n",
    "    \n",
    "        sw = func(y_CVtrain)\n",
    "        wt = func(y_CVtest)\n",
    "        \n",
    "        cv_p_clf.fit(CV_train, y_CVtrain, sample_weight = sw)\n",
    "        cv_p_pred = cv_p_clf.predict(CV_test)\n",
    "        \n",
    "        p_prec = precision_score(y_CVtest, cv_p_pred, labels=None, pos_label=1, average='binary', sample_weight=wt)\n",
    "        p_rec = recall_score(y_CVtest, cv_p_pred, labels=None, pos_label=1, average='binary', sample_weight=wt)\n",
    "        p_f = f1_score(y_CVtest, cv_p_pred, labels=None, pos_label=1, average='binary', sample_weight=wt)\n",
    "        \n",
    "        \n",
    "        p_precision.append(p_prec)\n",
    "        p_recall.append(p_rec)\n",
    "        p_f1.append(p_f)\n",
    "        \n",
    "        \n",
    "print (\"Perceptron Cross validation results: \")\n",
    "print (\"Precision: %.2f \"%(np.mean(p_precision)))\n",
    "print (\"   Recall: %.2f \" %(np.mean(p_recall)))\n",
    "print (\"       F1: %.2f \" %(np.mean(p_f1)))\n",
    "\n",
    "print (\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
