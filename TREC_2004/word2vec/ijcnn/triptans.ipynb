{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Required Libraries loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CR107\\Anaconda2\\lib\\site-packages\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\Users\\CR107\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#%run 'word2vec_data.ipynb'\n",
    "import cPickle as pickle\n",
    "%pylab inline\n",
    "\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import plotly.plotly as py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim import models\n",
    "\n",
    "\n",
    "import math\n",
    "import codecs\n",
    "import re\n",
    "import string\n",
    "from time import time\n",
    "\n",
    "\n",
    "## Sklearn\n",
    "from sklearn import svm, datasets, preprocessing, metrics\n",
    "import sklearn\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score, roc_auc_score, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score, precision_score\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets, feature_selection, cluster, feature_extraction, decomposition, metrics, model_selection\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.svm import LinearSVC\n",
    "from scipy import interp\n",
    "\n",
    "#############################\n",
    "\n",
    "#############################\n",
    "### other stuff\n",
    "from IPython import display\n",
    "\n",
    "import os\n",
    "#import ipyparallel as ipp\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# This is here to avoid all the Matplotlib warnings due to current bugs \n",
    "# - not a good idea to keep around\n",
    "print (\"Required Libraries loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "Software versions": [
        {
         "module": "Python",
         "version": "2.7.13 64bit [MSC v.1500 64 bit (AMD64)]"
        },
        {
         "module": "IPython",
         "version": "5.1.0"
        },
        {
         "module": "OS",
         "version": "Windows 7 6.1.7601 SP1"
        },
        {
         "module": "scipy",
         "version": "0.18.1"
        },
        {
         "module": "numpy",
         "version": "1.11.3"
        },
        {
         "module": "sklearn",
         "version": "0.18.1"
        },
        {
         "module": "pandas",
         "version": "0.19.2"
        },
        {
         "module": "scipy",
         "version": "0.18.1"
        },
        {
         "module": "nltk",
         "version": "3.2.2"
        },
        {
         "module": "gensim",
         "version": "1.0.1"
        },
        {
         "module": "matplotlib",
         "version": "1.5.1"
        },
        {
         "module": "os",
         "version": "The 'os' distribution was not found and is required by the application"
        },
        {
         "module": "cpickle",
         "version": "The 'cpickle' distribution was not found and is required by the application"
        }
       ]
      },
      "text/html": [
       "<table><tr><th>Software</th><th>Version</th></tr><tr><td>Python</td><td>2.7.13 64bit [MSC v.1500 64 bit (AMD64)]</td></tr><tr><td>IPython</td><td>5.1.0</td></tr><tr><td>OS</td><td>Windows 7 6.1.7601 SP1</td></tr><tr><td>scipy</td><td>0.18.1</td></tr><tr><td>numpy</td><td>1.11.3</td></tr><tr><td>sklearn</td><td>0.18.1</td></tr><tr><td>pandas</td><td>0.19.2</td></tr><tr><td>scipy</td><td>0.18.1</td></tr><tr><td>nltk</td><td>3.2.2</td></tr><tr><td>gensim</td><td>1.0.1</td></tr><tr><td>matplotlib</td><td>1.5.1</td></tr><tr><td>os</td><td>The 'os' distribution was not found and is required by the application</td></tr><tr><td>cpickle</td><td>The 'cpickle' distribution was not found and is required by the application</td></tr><tr><td colspan='2'>Tue Apr 04 14:27:28 2017 GMT Daylight Time</td></tr></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{|l|l|}\\hline\n",
       "{\\bf Software} & {\\bf Version} \\\\ \\hline\\hline\n",
       "Python & 2.7.13 64bit [MSC v.1500 64 bit (AMD64)] \\\\ \\hline\n",
       "IPython & 5.1.0 \\\\ \\hline\n",
       "OS & Windows 7 6.1.7601 SP1 \\\\ \\hline\n",
       "scipy & 0.18.1 \\\\ \\hline\n",
       "numpy & 1.11.3 \\\\ \\hline\n",
       "sklearn & 0.18.1 \\\\ \\hline\n",
       "pandas & 0.19.2 \\\\ \\hline\n",
       "scipy & 0.18.1 \\\\ \\hline\n",
       "nltk & 3.2.2 \\\\ \\hline\n",
       "gensim & 1.0.1 \\\\ \\hline\n",
       "matplotlib & 1.5.1 \\\\ \\hline\n",
       "os & The 'os' distribution was not found and is required by the application \\\\ \\hline\n",
       "cpickle & The 'cpickle' distribution was not found and is required by the application \\\\ \\hline\n",
       "\\hline \\multicolumn{2}{|l|}{Tue Apr 04 14:27:28 2017 GMT Daylight Time} \\\\ \\hline\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "Software versions\n",
       "Python 2.7.13 64bit [MSC v.1500 64 bit (AMD64)]\n",
       "IPython 5.1.0\n",
       "OS Windows 7 6.1.7601 SP1\n",
       "scipy 0.18.1\n",
       "numpy 1.11.3\n",
       "sklearn 0.18.1\n",
       "pandas 0.19.2\n",
       "scipy 0.18.1\n",
       "nltk 3.2.2\n",
       "gensim 1.0.1\n",
       "matplotlib 1.5.1\n",
       "os The 'os' distribution was not found and is required by the application\n",
       "cpickle The 'cpickle' distribution was not found and is required by the application\n",
       "Tue Apr 04 14:27:28 2017 GMT Daylight Time"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install --upgrade version_information\n",
    "#%reload_ext version_information\n",
    "%load_ext version_information \n",
    "%version_information scipy, numpy, sklearn, pandas, scipy, nltk, gensim, matplotlib, os, cpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inhibitor  = pd.read_csv(\"C:/EPC_Data/complete_data/raw/Triptans.csv\", sep=\",\", index_col = \"PMID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study</th>\n",
       "      <th>EndNote_Id</th>\n",
       "      <th>init_screen</th>\n",
       "      <th>fin_screen</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>MeSH</th>\n",
       "      <th>Mh_MeSH</th>\n",
       "      <th>Pt_Pub_Type</th>\n",
       "      <th>Pub_Type</th>\n",
       "      <th>TiAbs</th>\n",
       "      <th>TiAbsMeSHPT</th>\n",
       "      <th>TiAbsMeSHPT_append</th>\n",
       "      <th>TiAbsMeSH_append</th>\n",
       "      <th>TiAbsMesh</th>\n",
       "      <th>Title</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PMID</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12383060</th>\n",
       "      <td>Triptans</td>\n",
       "      <td>2159</td>\n",
       "      <td>E</td>\n",
       "      <td>E</td>\n",
       "      <td>The triptans, selective serotonin 5-HT1B/1D a...</td>\n",
       "      <td>Administration, Oral Clinical Trials as Topic...</td>\n",
       "      <td>[MHAdministration,, MHOral, MHClinical, MHTria...</td>\n",
       "      <td>[PTJournal, PTArticle, PTMeta-Analysis, PTRese...</td>\n",
       "      <td>Journal Article Meta-Analysis Research Suppor...</td>\n",
       "      <td>Triptans (serotonin, 5-HT1B/1D agonists) in mi...</td>\n",
       "      <td>Triptans (serotonin, 5-HT1B/1D agonists) in mi...</td>\n",
       "      <td>Triptans (serotonin, 5-HT1B/1D agonists) in mi...</td>\n",
       "      <td>Triptans (serotonin, 5-HT1B/1D agonists) in mi...</td>\n",
       "      <td>Triptans (serotonin, 5-HT1B/1D agonists) in mi...</td>\n",
       "      <td>Triptans (serotonin, 5-HT1B/1D agonists) in mi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11723269</th>\n",
       "      <td>Triptans</td>\n",
       "      <td>129</td>\n",
       "      <td>I</td>\n",
       "      <td>E</td>\n",
       "      <td>To assess the efficacy and tolerability of or...</td>\n",
       "      <td>Acute Disease Administration, Oral Adult Dose...</td>\n",
       "      <td>[MHAcute, MHDisease, MHAdministration,, MHOral...</td>\n",
       "      <td>[PTClinical, PTTrial, PTClinical, PTTrial,, PT...</td>\n",
       "      <td>Clinical Trial Clinical Trial, Phase II Compa...</td>\n",
       "      <td>Dose finding, placebo-controlled study of oral...</td>\n",
       "      <td>Dose finding, placebo-controlled study of oral...</td>\n",
       "      <td>Dose finding, placebo-controlled study of oral...</td>\n",
       "      <td>Dose finding, placebo-controlled study of oral...</td>\n",
       "      <td>Dose finding, placebo-controlled study of oral...</td>\n",
       "      <td>Dose finding, placebo-controlled study of oral...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             study  EndNote_Id init_screen fin_screen  \\\n",
       "PMID                                                    \n",
       "12383060  Triptans        2159           E          E   \n",
       "11723269  Triptans         129           I          E   \n",
       "\n",
       "                                                   Abstract  \\\n",
       "PMID                                                          \n",
       "12383060   The triptans, selective serotonin 5-HT1B/1D a...   \n",
       "11723269   To assess the efficacy and tolerability of or...   \n",
       "\n",
       "                                                       MeSH  \\\n",
       "PMID                                                          \n",
       "12383060   Administration, Oral Clinical Trials as Topic...   \n",
       "11723269   Acute Disease Administration, Oral Adult Dose...   \n",
       "\n",
       "                                                    Mh_MeSH  \\\n",
       "PMID                                                          \n",
       "12383060  [MHAdministration,, MHOral, MHClinical, MHTria...   \n",
       "11723269  [MHAcute, MHDisease, MHAdministration,, MHOral...   \n",
       "\n",
       "                                                Pt_Pub_Type  \\\n",
       "PMID                                                          \n",
       "12383060  [PTJournal, PTArticle, PTMeta-Analysis, PTRese...   \n",
       "11723269  [PTClinical, PTTrial, PTClinical, PTTrial,, PT...   \n",
       "\n",
       "                                                   Pub_Type  \\\n",
       "PMID                                                          \n",
       "12383060   Journal Article Meta-Analysis Research Suppor...   \n",
       "11723269   Clinical Trial Clinical Trial, Phase II Compa...   \n",
       "\n",
       "                                                      TiAbs  \\\n",
       "PMID                                                          \n",
       "12383060  Triptans (serotonin, 5-HT1B/1D agonists) in mi...   \n",
       "11723269  Dose finding, placebo-controlled study of oral...   \n",
       "\n",
       "                                                TiAbsMeSHPT  \\\n",
       "PMID                                                          \n",
       "12383060  Triptans (serotonin, 5-HT1B/1D agonists) in mi...   \n",
       "11723269  Dose finding, placebo-controlled study of oral...   \n",
       "\n",
       "                                         TiAbsMeSHPT_append  \\\n",
       "PMID                                                          \n",
       "12383060  Triptans (serotonin, 5-HT1B/1D agonists) in mi...   \n",
       "11723269  Dose finding, placebo-controlled study of oral...   \n",
       "\n",
       "                                           TiAbsMeSH_append  \\\n",
       "PMID                                                          \n",
       "12383060  Triptans (serotonin, 5-HT1B/1D agonists) in mi...   \n",
       "11723269  Dose finding, placebo-controlled study of oral...   \n",
       "\n",
       "                                                  TiAbsMesh  \\\n",
       "PMID                                                          \n",
       "12383060  Triptans (serotonin, 5-HT1B/1D agonists) in mi...   \n",
       "11723269  Dose finding, placebo-controlled study of oral...   \n",
       "\n",
       "                                                      Title  Label  \n",
       "PMID                                                                \n",
       "12383060  Triptans (serotonin, 5-HT1B/1D agonists) in mi...      0  \n",
       "11723269  Dose finding, placebo-controlled study of oral...      0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 29\n",
    "# shuffle dataset and split to train and test\n",
    "inhibitor = shuffle(inhibitor, random_state = seed)\n",
    "inhibitor.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.05524512  0.04734685 -0.16632569 -0.02529137 -0.0414103  -0.05263429\n",
      "  0.0091416  -0.02765045 -0.00656485  0.01953159]\n",
      "('\\nMatrix of word vectors is size : ', (1482L, 121L))\n"
     ]
    }
   ],
   "source": [
    "folder = \"C:/EPC_Data/complete_data/word2vec\"\n",
    "filepath = os.path.join(folder, \"Triptans_chi2_features_10minwords_15context\")\n",
    "model = models.Word2Vec.load(filepath)\n",
    "print (model[\"clinical\"][:10])\n",
    "print (\"\\nMatrix of word vectors is size : \", model.wv.syn0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create feature vectors from the averages of word vectors\n",
    "\n",
    "def makeFeatureVec(words, model):\n",
    "    \"\"\"\n",
    "    words - list of words (i.e. article) to be used as input for the creation of word vectors\n",
    "    model - model to use for the creation of the vectors\n",
    "    \n",
    "    makeFeatureVec: Function to average all of the word vectors in a given paragraph\n",
    "    returns: a numpy array of floats that are the average of the constituent word vectors for each word\n",
    "    \"\"\"\n",
    "    num_features = model.wv.syn0.shape[1]\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    #\n",
    "    # Loop over each word in the article and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(articles, model):\n",
    "    \"\"\"\n",
    "    articles - list of articles for which the creation of word vectors needs to be done for each\n",
    "    model - model to use for the creation of the vectors\n",
    "    \n",
    "    getAvgFeatureVecs: Given a set of articles (each one a list of words), calculate \n",
    "    the average feature vector for each one and return a 2D numpy array \n",
    "    returns: a 2D numpy array that contains the average of the constituent word vectors for each article\n",
    "    \"\"\"\n",
    "    num_features = model.wv.syn0.shape[1]\n",
    "    articleFeatureVecs = np.zeros((len(articles),num_features),dtype=\"float32\")\n",
    "    counter = 0.\n",
    "    \n",
    "    # Loop through the articles\n",
    "    for article in articles:\n",
    "        #\n",
    "        # Print a status message every 1000th review\n",
    "        if counter%500. == 0.:\n",
    "            print (\"Article %d of %d\" % (counter, len(articles)))\n",
    "        # \n",
    "        # Call the function (defined above) that makes average feature vectors\n",
    "        articleFeatureVecs[counter] = makeFeatureVec(article, model)\n",
    "        #\n",
    "        # Increment the counter\n",
    "        counter = counter + 1.\n",
    "    return articleFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import articles for string cleaning\n",
    "\n",
    "def clean_text(text, stem_words = False, remove_stopwords = False):\n",
    "    ###clean\n",
    "    texts = re.sub(\"[^a-zA-Z]\",    #pattern to match\n",
    "              \" \",              #replace other with this\n",
    "              str(text))                 #text to apply to\n",
    "         \n",
    "    #print \"Text recived: \", texts \n",
    "    clean_corpus = texts.lower().split()\n",
    "    #print \"corpus: \", clean_corpus\n",
    "    if stem_words:\n",
    "        # Porter stemmer\n",
    "        porter = nltk.PorterStemmer()\n",
    "        # Snowball stemmer\n",
    "        snowball = nltk.SnowballStemmer('english')\n",
    "        # Lancaster stemmer\n",
    "        lancaster = nltk.LancasterStemmer()\n",
    "        # General stemming Lambda function to stem tokens\n",
    "        clean_corpus = lambda tokens: [porter.stem(w) for w in corpus]\n",
    "    if remove_stopwords:   # Optionally remove stop words\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        clean_corpus = [w for w in clean_corpus if not w in stops]\n",
    "        #print \"Clean_corpus: \", clean_corpus\n",
    "    \n",
    "    return (clean_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average feature vecs for articles\n",
      "Article 0 of 671\n",
      "Article 500 of 671\n"
     ]
    }
   ],
   "source": [
    "# ****************************************************************\n",
    "# Calculate average feature vectors for training and testing sets,\n",
    "# using the functions we defined above. We remove stopwords.\n",
    "\n",
    "print (\"Creating average feature vecs for articles\")\n",
    "cleaned_articles = []\n",
    "for article in inhibitor.TiAbsMesh:\n",
    "    cleaned_articles.append(clean_text(article, stem_words=False, remove_stopwords=True ))\n",
    "\n",
    "averageWordVecs = getAvgFeatureVecs(cleaned_articles, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469\n",
      "202\n"
     ]
    }
   ],
   "source": [
    "RANDOM_STATE = 55\n",
    "TEST_SIZE = 0.3\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(averageWordVecs, inhibitor.Label, test_size=TEST_SIZE, \n",
    "                                                    random_state=37)\n",
    "\n",
    "print (len(X_train))\n",
    "print (len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0 647]\n",
      " [  1  24]]\n"
     ]
    }
   ],
   "source": [
    "X = averageWordVecs\n",
    "y = inhibitor.Label\n",
    "le = preprocessing.LabelEncoder()\n",
    "y  = le.fit_transform(y)\n",
    "yTrFreq = scipy.stats.itemfreq(inhibitor.Label)\n",
    "print(yTrFreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight(x):\n",
    "    x = np.where(x==0, 1, 4)\n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for recall\n",
      "()\n",
      "Best parameters set found on development set:\n",
      "()\n",
      "{'kernel': 'linear', 'C': 100, 'class_weight': 'balanced'}\n",
      "()\n",
      "Grid scores on development set:\n",
      "()\n",
      "0.500 (+/-0.000) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.001, 'class_weight': 'balanced'}\n",
      "0.500 (+/-0.000) for {'kernel': 'sigmoid', 'C': 1, 'gamma': 0.001, 'class_weight': 'balanced'}\n",
      "0.500 (+/-0.000) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.0001, 'class_weight': 'balanced'}\n",
      "0.500 (+/-0.000) for {'kernel': 'sigmoid', 'C': 1, 'gamma': 0.0001, 'class_weight': 'balanced'}\n",
      "0.500 (+/-0.000) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.001, 'class_weight': None}\n",
      "0.500 (+/-0.000) for {'kernel': 'sigmoid', 'C': 1, 'gamma': 0.001, 'class_weight': None}\n",
      "0.500 (+/-0.000) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.0001, 'class_weight': None}\n",
      "0.500 (+/-0.000) for {'kernel': 'sigmoid', 'C': 1, 'gamma': 0.0001, 'class_weight': None}\n",
      "0.500 (+/-0.000) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.001, 'class_weight': 'balanced'}\n",
      "0.500 (+/-0.000) for {'kernel': 'sigmoid', 'C': 10, 'gamma': 0.001, 'class_weight': 'balanced'}\n",
      "0.500 (+/-0.000) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.0001, 'class_weight': 'balanced'}\n",
      "0.500 (+/-0.000) for {'kernel': 'sigmoid', 'C': 10, 'gamma': 0.0001, 'class_weight': 'balanced'}\n",
      "0.500 (+/-0.000) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.001, 'class_weight': None}\n",
      "0.500 (+/-0.000) for {'kernel': 'sigmoid', 'C': 10, 'gamma': 0.001, 'class_weight': None}\n",
      "0.500 (+/-0.000) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.0001, 'class_weight': None}\n",
      "0.500 (+/-0.000) for {'kernel': 'sigmoid', 'C': 10, 'gamma': 0.0001, 'class_weight': None}\n",
      "0.668 (+/-0.034) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.001, 'class_weight': 'balanced'}\n",
      "0.684 (+/-0.010) for {'kernel': 'sigmoid', 'C': 100, 'gamma': 0.001, 'class_weight': 'balanced'}\n",
      "0.590 (+/-0.179) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.0001, 'class_weight': 'balanced'}\n",
      "0.590 (+/-0.179) for {'kernel': 'sigmoid', 'C': 100, 'gamma': 0.0001, 'class_weight': 'balanced'}\n",
      "0.500 (+/-0.000) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.001, 'class_weight': None}\n",
      "0.500 (+/-0.000) for {'kernel': 'sigmoid', 'C': 100, 'gamma': 0.001, 'class_weight': None}\n",
      "0.500 (+/-0.000) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.0001, 'class_weight': None}\n",
      "0.500 (+/-0.000) for {'kernel': 'sigmoid', 'C': 100, 'gamma': 0.0001, 'class_weight': None}\n",
      "0.746 (+/-0.061) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.001, 'class_weight': 'balanced'}\n",
      "0.737 (+/-0.061) for {'kernel': 'sigmoid', 'C': 1000, 'gamma': 0.001, 'class_weight': 'balanced'}\n",
      "0.668 (+/-0.034) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.0001, 'class_weight': 'balanced'}\n",
      "0.684 (+/-0.010) for {'kernel': 'sigmoid', 'C': 1000, 'gamma': 0.0001, 'class_weight': 'balanced'}\n",
      "0.500 (+/-0.000) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.001, 'class_weight': None}\n",
      "0.500 (+/-0.000) for {'kernel': 'sigmoid', 'C': 1000, 'gamma': 0.001, 'class_weight': None}\n",
      "0.500 (+/-0.000) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.0001, 'class_weight': None}\n",
      "0.500 (+/-0.000) for {'kernel': 'sigmoid', 'C': 1000, 'gamma': 0.0001, 'class_weight': None}\n",
      "0.737 (+/-0.061) for {'kernel': 'linear', 'C': 1, 'class_weight': 'balanced'}\n",
      "0.500 (+/-0.000) for {'kernel': 'linear', 'C': 1, 'class_weight': None}\n",
      "0.761 (+/-0.043) for {'kernel': 'linear', 'C': 10, 'class_weight': 'balanced'}\n",
      "0.500 (+/-0.000) for {'kernel': 'linear', 'C': 10, 'class_weight': None}\n",
      "0.782 (+/-0.014) for {'kernel': 'linear', 'C': 100, 'class_weight': 'balanced'}\n",
      "0.500 (+/-0.000) for {'kernel': 'linear', 'C': 100, 'class_weight': None}\n",
      "0.778 (+/-0.075) for {'kernel': 'linear', 'C': 1000, 'class_weight': 'balanced'}\n",
      "0.500 (+/-0.000) for {'kernel': 'linear', 'C': 1000, 'class_weight': None}\n",
      "()\n",
      "Detailed classification report:\n",
      "()\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "()\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      0.85      0.91       196\n",
      "          1       0.14      0.83      0.24         6\n",
      "\n",
      "avg / total       0.97      0.85      0.89       202\n",
      "\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf', \"sigmoid\"], 'gamma': [1e-3, 1e-4],\n",
    "                     'class_weight':['balanced', None],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000], 'class_weight':['balanced', None]}]\n",
    "\n",
    "scores = ['recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(SVC(), tuned_parameters, cv=2, scoring='%s_macro' % score)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clf.predict(X_test)\n",
    "    print(metrics.classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting for Seed = 35\n",
      "[[127 197]\n",
      " [  0  12]]\n",
      "Fitting for Seed = 35\n",
      "[[140 183]\n",
      " [  1  11]]\n",
      "Fitting for Seed = 71\n",
      "[[146 178]\n",
      " [  0  12]]\n",
      "Fitting for Seed = 71\n",
      "[[ 97 226]\n",
      " [  0  12]]\n",
      "Fitting for Seed = 21\n",
      "[[123 201]\n",
      " [  0  12]]\n",
      "Fitting for Seed = 21\n",
      "[[161 162]\n",
      " [  1  11]]\n",
      "Fitting for Seed = 61\n",
      "[[139 185]\n",
      " [  0  12]]\n",
      "Fitting for Seed = 61\n",
      "[[124 199]\n",
      " [  0  12]]\n",
      "Fitting for Seed = 55\n",
      "[[187 137]\n",
      " [  2  10]]\n",
      "Fitting for Seed = 55\n",
      "[[110 213]\n",
      " [  0  12]]\n",
      "SVM Cross validation results: \n",
      "Mean Precision: 0.06 +/- 0.00 \n",
      "  Mean  Recall: 0.97 +/- 0.06\n",
      "      Mean  F1: 0.11 +/- 0.01\n",
      "Mean  Accuracy: 0.44 +/- 0.07\n",
      "(' True Negative: ', [127, 140, 146, 97, 123, 161, 139, 124, 187, 110])\n",
      "('False Negative: ', [0, 1, 0, 0, 0, 1, 0, 0, 2, 0])\n",
      "(' True Positive: ', [12, 11, 12, 12, 12, 11, 12, 12, 10, 12])\n",
      "('False Positive: ', [197, 183, 178, 226, 201, 162, 185, 199, 137, 213])\n",
      "('Negative support Vectors: ', array([ 296.,  283.,  282.,  299.,  297.,  269.,  307.,  277.,  253.,  301.]))\n",
      "('Positive support Vectors: ', array([ 4.,  4.,  4.,  4.,  5.,  4.,  5.,  5.,  4.,  5.]))\n",
      "Mean positive support vectors: 4.40 +/- 0.49\n",
      "Mean Negative support vectors: 286.40 +/- 15.93\n",
      "Train positive: 12.00\n",
      "Train negattive: 323.50\n",
      "Test positive: 12.00\n",
      "Test negative: 323.50\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(C=1.0, cache_size=200, class_weight='balanced', coef0=0.0,decision_function_shape=None, degree=3, \n",
    "              gamma='auto', kernel='linear',max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
    "              tol=0.001, verbose=False)\n",
    "\n",
    "\n",
    "seeds = [35, 71, 21, 61, 55]\n",
    "\n",
    "svm_precision = []\n",
    "svm_recall = []\n",
    "svm_f = []\n",
    "acc = []\n",
    "ps_vectors = []\n",
    "ns_vectors = []\n",
    "tr_pos = []\n",
    "tr_neg = []\n",
    "te_pos = []\n",
    "te_neg = []\n",
    "true_pos = []\n",
    "true_neg = []\n",
    "false_pos = []\n",
    "false_neg = []\n",
    "\n",
    "\n",
    "for i in range(0, 5):\n",
    "    \n",
    "    seed = seeds[i]   \n",
    "    skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=seed)\n",
    "\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        \n",
    "           \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        w = weight(y_train)\n",
    "        yTrFreq = scipy.stats.itemfreq(y_train)\n",
    "        tr_pos = np.append(tr_pos, yTrFreq[1][1])\n",
    "        tr_neg = np.append(tr_neg, yTrFreq[0][1])\n",
    "\n",
    "    # print(yTrFreq)\n",
    "        yTrFreq = scipy.stats.itemfreq(y_test)\n",
    "        te_pos = np.append(te_pos, yTrFreq[1][1])\n",
    "        te_neg = np.append(te_neg, yTrFreq[0][1])\n",
    "        \n",
    "        print (\"Fitting for Seed = %d\" % seed) \n",
    "\n",
    "        clf.fit(X_train, y_train, sample_weight = w)\n",
    "        pred = clf.predict(X_test)\n",
    " \n",
    "               \n",
    "        s_vector = clf.n_support_\n",
    "        s_prec = metrics.precision_score(y_test, pred)#, sample_weight=wt)\n",
    "        s_recall = metrics.recall_score(y_test, pred)#, sample_weight=wt)\n",
    "        s_f1 = metrics.f1_score(y_test, pred)#, sample_weight=wt)\n",
    "        acc_s = metrics.accuracy_score(y_test, pred)\n",
    "\n",
    "        print (metrics.confusion_matrix(y_test, pred))\n",
    "        true_neg.append(metrics.confusion_matrix(y_test, pred)[0,0])\n",
    "        true_pos.append(metrics.confusion_matrix(y_test, pred)[1,1])\n",
    "        false_neg.append(metrics.confusion_matrix(y_test, pred)[1, 0])\n",
    "        false_pos.append(metrics.confusion_matrix(y_test, pred)[0, 1])\n",
    "\n",
    "\n",
    "        svm_precision.append(s_prec)\n",
    "        svm_recall.append(s_recall)\n",
    "        svm_f.append(s_f1)\n",
    "        acc.append(acc_s)\n",
    "        ps_vectors = np.append(ps_vectors, s_vector[1])\n",
    "        ns_vectors = np.append(ns_vectors, s_vector[0])\n",
    "\n",
    "print (\"SVM Cross validation results: \")\n",
    "print (\"Mean Precision: %.2f +/- %.2f \"%(np.mean(svm_precision), np.std(svm_precision)))\n",
    "print (\"  Mean  Recall: %.2f +/- %.2f\" %(np.mean(svm_recall), np.std(svm_recall)))\n",
    "print (\"      Mean  F1: %.2f +/- %.2f\" %(np.mean(svm_f), np.std(svm_f)))\n",
    "print (\"Mean  Accuracy: %.2f +/- %.2f\" %(np.mean(acc), np.std(acc)))\n",
    "print (\" True Negative: \", true_neg)\n",
    "print (\"False Negative: \", false_neg)\n",
    "print (\" True Positive: \", true_pos)\n",
    "print (\"False Positive: \", false_pos)\n",
    "print (\"Negative support Vectors: \", ns_vectors)\n",
    "print (\"Positive support Vectors: \", ps_vectors)\n",
    "print (\"Mean positive support vectors: %.2f +/- %.2f\"%(np.mean(ps_vectors), np.std(ps_vectors)))\n",
    "print (\"Mean Negative support vectors: %.2f +/- %.2f\"%(np.mean(ns_vectors), np.std(ns_vectors)))\n",
    "print (\"Train positive: %.2f\" %(np.mean(tr_pos)))\n",
    "print (\"Train negattive: %.2f\" %(np.mean(tr_neg)))\n",
    "print (\"Test positive: %.2f\" %(np.mean(te_pos)))\n",
    "print (\"Test negative: %.2f\" %(np.mean(te_neg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting for Seed = 35\n",
      "[[220 104]\n",
      " [  0  12]]\n",
      "Fitting for Seed = 35\n",
      "[[203 120]\n",
      " [  1  11]]\n",
      "Fitting for Seed = 71\n",
      "[[208 116]\n",
      " [  2  10]]\n",
      "Fitting for Seed = 71\n",
      "[[198 125]\n",
      " [  0  12]]\n",
      "Fitting for Seed = 21\n",
      "[[236  88]\n",
      " [  3   9]]\n",
      "Fitting for Seed = 21\n",
      "[[208 115]\n",
      " [  1  11]]\n",
      "Fitting for Seed = 61\n",
      "[[214 110]\n",
      " [  0  12]]\n",
      "Fitting for Seed = 61\n",
      "[[203 120]\n",
      " [  1  11]]\n",
      "Fitting for Seed = 55\n",
      "[[226  98]\n",
      " [  3   9]]\n",
      "Fitting for Seed = 55\n",
      "[[194 129]\n",
      " [  1  11]]\n",
      "SVM Cross validation results: \n",
      "Mean Precision: 0.09 +/- 0.01 \n",
      "  Mean  Recall: 0.90 +/- 0.09\n",
      "      Mean  F1: 0.16 +/- 0.01\n",
      "Mean  Accuracy: 0.66 +/- 0.03\n",
      "(' True Negative: ', [220, 203, 208, 198, 236, 208, 214, 203, 226, 194])\n",
      "('False Negative: ', [0, 1, 2, 0, 3, 1, 0, 1, 3, 1])\n",
      "(' True Positive: ', [12, 11, 10, 12, 9, 11, 12, 11, 9, 11])\n",
      "('False Positive: ', [104, 120, 116, 125, 88, 115, 110, 120, 98, 129])\n",
      "('Negative support Vectors: ', array([ 190.,  199.,  195.,  195.,  164.,  186.,  193.,  196.,  156.,  204.]))\n",
      "('Positive support Vectors: ', array([ 5.,  3.,  3.,  5.,  6.,  4.,  4.,  3.,  4.,  4.]))\n",
      "Mean positive support vectors: 4.10 +/- 0.94\n",
      "Mean Negative support vectors: 187.80 +/- 14.74\n",
      "Train positive: 12.00\n",
      "Train negattive: 323.50\n",
      "Test positive: 12.00\n",
      "Test negative: 323.50\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(C=10.0, cache_size=200, class_weight='balanced', coef0=0.0,decision_function_shape=None, degree=3, \n",
    "              gamma='auto', kernel='linear', max_iter=-1, probability=False, random_state=55, shrinking=True,\n",
    "              tol=0.001, verbose=False)\n",
    "\n",
    "seeds = [35, 71, 21, 61, 55]\n",
    "\n",
    "svm_precision = []\n",
    "svm_recall = []\n",
    "svm_f = []\n",
    "acc = []\n",
    "ps_vectors = []\n",
    "ns_vectors = []\n",
    "tr_pos = []\n",
    "tr_neg = []\n",
    "te_pos = []\n",
    "te_neg = []\n",
    "true_pos = []\n",
    "true_neg = []\n",
    "false_pos = []\n",
    "false_neg = []\n",
    "\n",
    "\n",
    "for i in range(0, 5):\n",
    "    \n",
    "    seed = seeds[i]   \n",
    "    skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=seed)\n",
    "\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        \n",
    "           \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        w = weight(y_train)\n",
    "        yTrFreq = scipy.stats.itemfreq(y_train)\n",
    "        tr_pos = np.append(tr_pos, yTrFreq[1][1])\n",
    "        tr_neg = np.append(tr_neg, yTrFreq[0][1])\n",
    "\n",
    "    # print(yTrFreq)\n",
    "        yTrFreq = scipy.stats.itemfreq(y_test)\n",
    "        te_pos = np.append(te_pos, yTrFreq[1][1])\n",
    "        te_neg = np.append(te_neg, yTrFreq[0][1])\n",
    "        \n",
    "        print (\"Fitting for Seed = %d\" % seed) \n",
    "\n",
    "        clf.fit(X_train, y_train, sample_weight = w)\n",
    "        pred = clf.predict(X_test)\n",
    " \n",
    "               \n",
    "        s_vector = clf.n_support_\n",
    "        s_prec = metrics.precision_score(y_test, pred)#, sample_weight=wt)\n",
    "        s_recall = metrics.recall_score(y_test, pred)#, sample_weight=wt)\n",
    "        s_f1 = metrics.f1_score(y_test, pred)#, sample_weight=wt)\n",
    "        acc_s = metrics.accuracy_score(y_test, pred)\n",
    "\n",
    "        print (metrics.confusion_matrix(y_test, pred))\n",
    "        true_neg.append(metrics.confusion_matrix(y_test, pred)[0,0])\n",
    "        true_pos.append(metrics.confusion_matrix(y_test, pred)[1,1])\n",
    "        false_neg.append(metrics.confusion_matrix(y_test, pred)[1, 0])\n",
    "        false_pos.append(metrics.confusion_matrix(y_test, pred)[0, 1])\n",
    "\n",
    "\n",
    "        svm_precision.append(s_prec)\n",
    "        svm_recall.append(s_recall)\n",
    "        svm_f.append(s_f1)\n",
    "        acc.append(acc_s)\n",
    "        ps_vectors = np.append(ps_vectors, s_vector[1])\n",
    "        ns_vectors = np.append(ns_vectors, s_vector[0])\n",
    "\n",
    "\n",
    "print (\"SVM Cross validation results: \")\n",
    "print (\"Mean Precision: %.2f +/- %.2f \"%(np.mean(svm_precision), np.std(svm_precision)))\n",
    "print (\"  Mean  Recall: %.2f +/- %.2f\" %(np.mean(svm_recall), np.std(svm_recall)))\n",
    "print (\"      Mean  F1: %.2f +/- %.2f\" %(np.mean(svm_f), np.std(svm_f)))\n",
    "print (\"Mean  Accuracy: %.2f +/- %.2f\" %(np.mean(acc), np.std(acc)))\n",
    "print (\" True Negative: \", true_neg)\n",
    "print (\"False Negative: \", false_neg)\n",
    "print (\" True Positive: \", true_pos)\n",
    "print (\"False Positive: \", false_pos)\n",
    "print (\"Negative support Vectors: \", ns_vectors)\n",
    "print (\"Positive support Vectors: \", ps_vectors)\n",
    "print (\"Mean positive support vectors: %.2f +/- %.2f\"%(np.mean(ps_vectors), np.std(ps_vectors)))\n",
    "print (\"Mean Negative support vectors: %.2f +/- %.2f\"%(np.mean(ns_vectors), np.std(ns_vectors)))\n",
    "print (\"Train positive: %.2f\" %(np.mean(tr_pos)))\n",
    "print (\"Train negattive: %.2f\" %(np.mean(tr_neg)))\n",
    "print (\"Test positive: %.2f\" %(np.mean(te_pos)))\n",
    "print (\"Test negative: %.2f\" %(np.mean(te_neg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting for Seed = 35\n",
      "[[114 210]\n",
      " [  0  12]]\n",
      "Fitting for Seed = 35\n",
      "[[125 198]\n",
      " [  0  12]]\n",
      "Fitting for Seed = 71\n",
      "[[131 193]\n",
      " [  0  12]]\n",
      "Fitting for Seed = 71\n",
      "[[ 86 237]\n",
      " [  0  12]]\n",
      "Fitting for Seed = 21\n",
      "[[112 212]\n",
      " [  0  12]]\n",
      "Fitting for Seed = 21\n",
      "[[155 168]\n",
      " [  0  12]]\n",
      "Fitting for Seed = 61\n",
      "[[122 202]\n",
      " [  0  12]]\n",
      "Fitting for Seed = 61\n",
      "[[105 218]\n",
      " [  0  12]]\n",
      "Fitting for Seed = 55\n",
      "[[172 152]\n",
      " [  2  10]]\n",
      "Fitting for Seed = 55\n",
      "[[100 223]\n",
      " [  0  12]]\n",
      "SVM Cross validation results: \n",
      "Mean Precision: 0.06 +/- 0.01 \n",
      "  Mean  Recall: 0.98 +/- 0.05\n",
      "      Mean  F1: 0.11 +/- 0.01\n",
      "Mean  Accuracy: 0.40 +/- 0.07\n",
      "(' True Negative: ', [114, 125, 131, 86, 112, 155, 122, 105, 172, 100])\n",
      "('False Negative: ', [0, 0, 0, 0, 0, 0, 0, 0, 2, 0])\n",
      "(' True Positive: ', [12, 12, 12, 12, 12, 12, 12, 12, 10, 12])\n",
      "('False Positive: ', [210, 198, 193, 237, 212, 168, 202, 218, 152, 223])\n",
      "('Negative support Vectors: ', array([ 301.,  290.,  288.,  302.,  302.,  274.,  312.,  281.,  264.,  306.]))\n",
      "('Positive support Vectors: ', array([ 4.,  4.,  4.,  4.,  4.,  3.,  5.,  5.,  4.,  5.]))\n",
      "Mean positive support vectors: 4.20 +/- 0.60\n",
      "Mean Negative support vectors: 292.00 +/- 14.58\n",
      "Train positive: 12.00\n",
      "Train negattive: 323.50\n",
      "Test positive: 12.00\n",
      "Test negative: 323.50\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(C=100.0, cache_size=200, class_weight='balanced', coef0=0.0,decision_function_shape=None, degree=3, \n",
    "              gamma='auto', kernel='sigmoid', max_iter=-1, probability=False, random_state=55, shrinking=True,\n",
    "              tol=0.001, verbose=False)\n",
    "\n",
    "seeds = [35, 71, 21, 61, 55]\n",
    "\n",
    "svm_precision = []\n",
    "svm_recall = []\n",
    "svm_f = []\n",
    "acc = []\n",
    "ps_vectors = []\n",
    "ns_vectors = []\n",
    "tr_pos = []\n",
    "tr_neg = []\n",
    "te_pos = []\n",
    "te_neg = []\n",
    "true_pos = []\n",
    "true_neg = []\n",
    "false_pos = []\n",
    "false_neg = []\n",
    "\n",
    "\n",
    "for i in range(0, 5):\n",
    "    \n",
    "    seed = seeds[i]   \n",
    "    skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=seed)\n",
    "\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        \n",
    "           \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        w = weight(y_train)\n",
    "        yTrFreq = scipy.stats.itemfreq(y_train)\n",
    "        tr_pos = np.append(tr_pos, yTrFreq[1][1])\n",
    "        tr_neg = np.append(tr_neg, yTrFreq[0][1])\n",
    "\n",
    "    # print(yTrFreq)\n",
    "        yTrFreq = scipy.stats.itemfreq(y_test)\n",
    "        te_pos = np.append(te_pos, yTrFreq[1][1])\n",
    "        te_neg = np.append(te_neg, yTrFreq[0][1])\n",
    "        \n",
    "        print (\"Fitting for Seed = %d\" % seed) \n",
    "\n",
    "        clf.fit(X_train, y_train, sample_weight = w)\n",
    "        pred = clf.predict(X_test)\n",
    " \n",
    "               \n",
    "        s_vector = clf.n_support_\n",
    "        s_prec = metrics.precision_score(y_test, pred)#, sample_weight=wt)\n",
    "        s_recall = metrics.recall_score(y_test, pred)#, sample_weight=wt)\n",
    "        s_f1 = metrics.f1_score(y_test, pred)#, sample_weight=wt)\n",
    "        acc_s = metrics.accuracy_score(y_test, pred)\n",
    "\n",
    "        print (metrics.confusion_matrix(y_test, pred))\n",
    "        true_neg.append(metrics.confusion_matrix(y_test, pred)[0,0])\n",
    "        true_pos.append(metrics.confusion_matrix(y_test, pred)[1,1])\n",
    "        false_neg.append(metrics.confusion_matrix(y_test, pred)[1, 0])\n",
    "        false_pos.append(metrics.confusion_matrix(y_test, pred)[0, 1])\n",
    "\n",
    "\n",
    "        svm_precision.append(s_prec)\n",
    "        svm_recall.append(s_recall)\n",
    "        svm_f.append(s_f1)\n",
    "        acc.append(acc_s)\n",
    "        ps_vectors = np.append(ps_vectors, s_vector[1])\n",
    "        ns_vectors = np.append(ns_vectors, s_vector[0])\n",
    "\n",
    "\n",
    "print (\"SVM Cross validation results: \")\n",
    "print (\"Mean Precision: %.2f +/- %.2f \"%(np.mean(svm_precision), np.std(svm_precision)))\n",
    "print (\"  Mean  Recall: %.2f +/- %.2f\" %(np.mean(svm_recall), np.std(svm_recall)))\n",
    "print (\"      Mean  F1: %.2f +/- %.2f\" %(np.mean(svm_f), np.std(svm_f)))\n",
    "print (\"Mean  Accuracy: %.2f +/- %.2f\" %(np.mean(acc), np.std(acc)))\n",
    "print (\" True Negative: \", true_neg)\n",
    "print (\"False Negative: \", false_neg)\n",
    "print (\" True Positive: \", true_pos)\n",
    "print (\"False Positive: \", false_pos)\n",
    "print (\"Negative support Vectors: \", ns_vectors)\n",
    "print (\"Positive support Vectors: \", ps_vectors)\n",
    "print (\"Mean positive support vectors: %.2f +/- %.2f\"%(np.mean(ps_vectors), np.std(ps_vectors)))\n",
    "print (\"Mean Negative support vectors: %.2f +/- %.2f\"%(np.mean(ns_vectors), np.std(ns_vectors)))\n",
    "print (\"Train positive: %.2f\" %(np.mean(tr_pos)))\n",
    "print (\"Train negattive: %.2f\" %(np.mean(tr_neg)))\n",
    "print (\"Test positive: %.2f\" %(np.mean(te_pos)))\n",
    "print (\"Test negative: %.2f\" %(np.mean(te_neg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting for Seed = 35\n",
      "[[178 146]\n",
      " [  0  12]]\n",
      "Fitting for Seed = 35\n",
      "[[163 160]\n",
      " [  1  11]]\n",
      "Fitting for Seed = 71\n",
      "[[167 157]\n",
      " [  0  12]]\n",
      "Fitting for Seed = 71\n",
      "[[133 190]\n",
      " [  0  12]]\n",
      "Fitting for Seed = 21\n",
      "[[174 150]\n",
      " [  0  12]]\n",
      "Fitting for Seed = 21\n",
      "[[173 150]\n",
      " [  1  11]]\n",
      "Fitting for Seed = 61\n",
      "[[175 149]\n",
      " [  0  12]]\n",
      "Fitting for Seed = 61\n",
      "[[141 182]\n",
      " [  1  11]]\n",
      "Fitting for Seed = 55\n",
      "[[196 128]\n",
      " [  3   9]]\n",
      "Fitting for Seed = 55\n",
      "[[137 186]\n",
      " [  0  12]]\n",
      "SVM Cross validation results: \n",
      "Mean Precision: 0.07 +/- 0.01 \n",
      "  Mean  Recall: 0.95 +/- 0.08\n",
      "      Mean  F1: 0.13 +/- 0.01\n",
      "Mean  Accuracy: 0.52 +/- 0.06\n",
      "(' True Negative: ', [178, 163, 167, 133, 174, 173, 175, 141, 196, 137])\n",
      "('False Negative: ', [0, 1, 0, 0, 0, 1, 0, 1, 3, 0])\n",
      "(' True Positive: ', [12, 11, 12, 12, 12, 11, 12, 11, 9, 12])\n",
      "('False Positive: ', [146, 160, 157, 190, 150, 150, 149, 182, 128, 186])\n",
      "('Negative support Vectors: ', array([ 284.,  259.,  259.,  287.,  273.,  251.,  290.,  257.,  225.,  284.]))\n",
      "('Positive support Vectors: ', array([ 4.,  4.,  4.,  4.,  6.,  5.,  5.,  4.,  4.,  4.]))\n",
      "Mean positive support vectors: 4.40 +/- 0.66\n",
      "Mean Negative support vectors: 266.90 +/- 19.47\n",
      "Train positive: 12.00\n",
      "Train negattive: 323.50\n",
      "Test positive: 12.00\n",
      "Test negative: 323.50\n"
     ]
    }
   ],
   "source": [
    "clf = SVC(C=100.0, cache_size=200, class_weight='balanced', coef0=0.0,decision_function_shape=None, degree=3, \n",
    "              gamma='auto', kernel='rbf', max_iter=-1, probability=False, random_state=55, shrinking=True,\n",
    "              tol=0.001, verbose=False)\n",
    "\n",
    "seeds = [35, 71, 21, 61, 55]\n",
    "\n",
    "svm_precision = []\n",
    "svm_recall = []\n",
    "svm_f = []\n",
    "acc = []\n",
    "ps_vectors = []\n",
    "ns_vectors = []\n",
    "tr_pos = []\n",
    "tr_neg = []\n",
    "te_pos = []\n",
    "te_neg = []\n",
    "true_pos = []\n",
    "true_neg = []\n",
    "false_pos = []\n",
    "false_neg = []\n",
    "\n",
    "\n",
    "for i in range(0, 5):\n",
    "    \n",
    "    seed = seeds[i]   \n",
    "    skf = StratifiedKFold(n_splits=2, shuffle=True, random_state=seed)\n",
    "\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        \n",
    "           \n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        w = weight(y_train)\n",
    "        yTrFreq = scipy.stats.itemfreq(y_train)\n",
    "        tr_pos = np.append(tr_pos, yTrFreq[1][1])\n",
    "        tr_neg = np.append(tr_neg, yTrFreq[0][1])\n",
    "\n",
    "    # print(yTrFreq)\n",
    "        yTrFreq = scipy.stats.itemfreq(y_test)\n",
    "        te_pos = np.append(te_pos, yTrFreq[1][1])\n",
    "        te_neg = np.append(te_neg, yTrFreq[0][1])\n",
    "        \n",
    "        print (\"Fitting for Seed = %d\" % seed) \n",
    "\n",
    "        clf.fit(X_train, y_train, sample_weight = w)\n",
    "        pred = clf.predict(X_test)\n",
    " \n",
    "               \n",
    "        s_vector = clf.n_support_\n",
    "        s_prec = metrics.precision_score(y_test, pred)#, sample_weight=wt)\n",
    "        s_recall = metrics.recall_score(y_test, pred)#, sample_weight=wt)\n",
    "        s_f1 = metrics.f1_score(y_test, pred)#, sample_weight=wt)\n",
    "        acc_s = metrics.accuracy_score(y_test, pred)\n",
    "\n",
    "        print (metrics.confusion_matrix(y_test, pred))\n",
    "        true_neg.append(metrics.confusion_matrix(y_test, pred)[0,0])\n",
    "        true_pos.append(metrics.confusion_matrix(y_test, pred)[1,1])\n",
    "        false_neg.append(metrics.confusion_matrix(y_test, pred)[1, 0])\n",
    "        false_pos.append(metrics.confusion_matrix(y_test, pred)[0, 1])\n",
    "\n",
    "\n",
    "        svm_precision.append(s_prec)\n",
    "        svm_recall.append(s_recall)\n",
    "        svm_f.append(s_f1)\n",
    "        acc.append(acc_s)\n",
    "        ps_vectors = np.append(ps_vectors, s_vector[1])\n",
    "        ns_vectors = np.append(ns_vectors, s_vector[0])\n",
    "\n",
    "\n",
    "print (\"SVM Cross validation results: \")\n",
    "print (\"Mean Precision: %.2f +/- %.2f \"%(np.mean(svm_precision), np.std(svm_precision)))\n",
    "print (\"  Mean  Recall: %.2f +/- %.2f\" %(np.mean(svm_recall), np.std(svm_recall)))\n",
    "print (\"      Mean  F1: %.2f +/- %.2f\" %(np.mean(svm_f), np.std(svm_f)))\n",
    "print (\"Mean  Accuracy: %.2f +/- %.2f\" %(np.mean(acc), np.std(acc)))\n",
    "print (\" True Negative: \", true_neg)\n",
    "print (\"False Negative: \", false_neg)\n",
    "print (\" True Positive: \", true_pos)\n",
    "print (\"False Positive: \", false_pos)\n",
    "print (\"Negative support Vectors: \", ns_vectors)\n",
    "print (\"Positive support Vectors: \", ps_vectors)\n",
    "print (\"Mean positive support vectors: %.2f +/- %.2f\"%(np.mean(ps_vectors), np.std(ps_vectors)))\n",
    "print (\"Mean Negative support vectors: %.2f +/- %.2f\"%(np.mean(ns_vectors), np.std(ns_vectors)))\n",
    "print (\"Train positive: %.2f\" %(np.mean(tr_pos)))\n",
    "print (\"Train negattive: %.2f\" %(np.mean(tr_neg)))\n",
    "print (\"Test positive: %.2f\" %(np.mean(te_pos)))\n",
    "print (\"Test negative: %.2f\" %(np.mean(te_neg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
