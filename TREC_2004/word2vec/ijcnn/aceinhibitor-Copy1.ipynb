{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "Required Libraries loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\raylite\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\raylite\\Anaconda2\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#%run 'word2vec_data.ipynb'\n",
    "import cPickle as pickle\n",
    "%pylab inline\n",
    "\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import plotly.plotly as py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from gensim import models\n",
    "\n",
    "\n",
    "import math\n",
    "import codecs\n",
    "import re\n",
    "import string\n",
    "from time import time\n",
    "\n",
    "\n",
    "## Sklearn\n",
    "from sklearn import svm, datasets\n",
    "import sklearn\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn import cross_validation, metrics\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score, roc_auc_score, accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, accuracy_score, precision_score\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, label_binarize, Imputer, OneHotEncoder\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier, BaggingClassifier, RandomTreesEmbedding\n",
    "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\n",
    "from sklearn import linear_model\n",
    "from sklearn import datasets, feature_selection, cluster, feature_extraction, grid_search, decomposition\n",
    "from sklearn import neighbors, decomposition, metrics, cross_validation, feature_selection, model_selection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import SGDClassifier, Perceptron, PassiveAggressiveClassifier, RidgeClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.utils.extmath import density\n",
    "from scipy import interp\n",
    "\n",
    "#############################\n",
    "### Matplotlib\n",
    "import matplotlib.pyplot as plt # module for plotting \n",
    "from matplotlib import interactive, font_manager\n",
    "from matplotlib import rcParams\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib as mpl\n",
    "from matplotlib.colors import ListedColormap\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "#############################\n",
    "### other stuff\n",
    "from IPython import display\n",
    "\n",
    "import os\n",
    "#import ipyparallel as ipp\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# This is here to avoid all the Matplotlib warnings due to current bugs \n",
    "# - not a good idea to keep around\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline  \n",
    "\n",
    "print (\"Required Libraries loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "Software versions": [
        {
         "module": "Python",
         "version": "2.7.13 64bit [MSC v.1500 64 bit (AMD64)]"
        },
        {
         "module": "IPython",
         "version": "5.1.0"
        },
        {
         "module": "OS",
         "version": "Windows 7 6.1.7601 SP1"
        },
        {
         "module": "scipy",
         "version": "0.18.1"
        },
        {
         "module": "numpy",
         "version": "1.11.2"
        },
        {
         "module": "sklearn",
         "version": "0.18.1"
        },
        {
         "module": "pandas",
         "version": "0.18.1"
        },
        {
         "module": "scipy",
         "version": "0.18.1"
        },
        {
         "module": "nltk",
         "version": "3.2.1"
        },
        {
         "module": "gensim",
         "version": "0.12.4"
        },
        {
         "module": "matplotlib",
         "version": "1.5.3"
        }
       ]
      },
      "text/html": [
       "<table><tr><th>Software</th><th>Version</th></tr><tr><td>Python</td><td>2.7.13 64bit [MSC v.1500 64 bit (AMD64)]</td></tr><tr><td>IPython</td><td>5.1.0</td></tr><tr><td>OS</td><td>Windows 7 6.1.7601 SP1</td></tr><tr><td>scipy</td><td>0.18.1</td></tr><tr><td>numpy</td><td>1.11.2</td></tr><tr><td>sklearn</td><td>0.18.1</td></tr><tr><td>pandas</td><td>0.18.1</td></tr><tr><td>scipy</td><td>0.18.1</td></tr><tr><td>nltk</td><td>3.2.1</td></tr><tr><td>gensim</td><td>0.12.4</td></tr><tr><td>matplotlib</td><td>1.5.3</td></tr><tr><td colspan='2'>Thu Dec 29 15:28:30 2016 GMT Standard Time</td></tr></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{|l|l|}\\hline\n",
       "{\\bf Software} & {\\bf Version} \\\\ \\hline\\hline\n",
       "Python & 2.7.13 64bit [MSC v.1500 64 bit (AMD64)] \\\\ \\hline\n",
       "IPython & 5.1.0 \\\\ \\hline\n",
       "OS & Windows 7 6.1.7601 SP1 \\\\ \\hline\n",
       "scipy & 0.18.1 \\\\ \\hline\n",
       "numpy & 1.11.2 \\\\ \\hline\n",
       "sklearn & 0.18.1 \\\\ \\hline\n",
       "pandas & 0.18.1 \\\\ \\hline\n",
       "scipy & 0.18.1 \\\\ \\hline\n",
       "nltk & 3.2.1 \\\\ \\hline\n",
       "gensim & 0.12.4 \\\\ \\hline\n",
       "matplotlib & 1.5.3 \\\\ \\hline\n",
       "\\hline \\multicolumn{2}{|l|}{Thu Dec 29 15:28:30 2016 GMT Standard Time} \\\\ \\hline\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "Software versions\n",
       "Python 2.7.13 64bit [MSC v.1500 64 bit (AMD64)]\n",
       "IPython 5.1.0\n",
       "OS Windows 7 6.1.7601 SP1\n",
       "scipy 0.18.1\n",
       "numpy 1.11.2\n",
       "sklearn 0.18.1\n",
       "pandas 0.18.1\n",
       "scipy 0.18.1\n",
       "nltk 3.2.1\n",
       "gensim 0.12.4\n",
       "matplotlib 1.5.3\n",
       "Thu Dec 29 15:28:30 2016 GMT Standard Time"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install --upgrade version_information\n",
    "#%reload_ext version_information\n",
    "%load_ext version_information \n",
    "%version_information scipy, numpy, sklearn, pandas, scipy, nltk, gensim, matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inhibitor  = pd.read_csv(\"C:\\EPC_Data\\TREC_BROKEN\\No_Mh_Tag/aceinhibitor_no_mh.csv\", sep=\",\", index_col = \"PMID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.05496166  0.05857657 -0.09459522 -0.01349878 -0.00286666 -0.05296639\n",
      " -0.0891526  -0.04481089  0.01484196 -0.00808054]\n",
      "('\\nMatrix of word vectors is size : ', (3900L, 1000L))\n"
     ]
    }
   ],
   "source": [
    "folder = \"C:\\Users\\CR107\\Dropbox\\PhD\\Experiments\\TREC_2004\\data\\word2vec\"\n",
    "folder2 = \"C:/Users/raylite/Dropbox/PhD/Experiments/TREC_2004/data/word2vec\"\n",
    "try:\n",
    "    filepath = os.path.join(folder, \"aceinhibitor_1000features_10minwords_15context\")\n",
    "    model = models.Word2Vec.load(filepath)\n",
    "except:\n",
    "    filepath = os.path.join(folder2, \"aceinhibitor_1000features_10minwords_15context\")\n",
    "    model = models.Word2Vec.load(filepath)\n",
    "print (model[\"clinical\"][:10])\n",
    "print (\"\\nMatrix of word vectors is size : \", model.syn0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create feature vectors from the averages of word vectors\n",
    "\n",
    "def makeFeatureVec(words, model):\n",
    "    \"\"\"\n",
    "    words - list of words (i.e. article) to be used as input for the creation of word vectors\n",
    "    model - model to use for the creation of the vectors\n",
    "    \n",
    "    makeFeatureVec: Function to average all of the word vectors in a given paragraph\n",
    "    returns: a numpy array of floats that are the average of the constituent word vectors for each word\n",
    "    \"\"\"\n",
    "    num_features = model.syn0.shape[1]\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.index2word)\n",
    "    #\n",
    "    # Loop over each word in the article and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(articles, model):\n",
    "    \"\"\"\n",
    "    articles - list of articles for which the creation of word vectors needs to be done for each\n",
    "    model - model to use for the creation of the vectors\n",
    "    \n",
    "    getAvgFeatureVecs: Given a set of articles (each one a list of words), calculate \n",
    "    the average feature vector for each one and return a 2D numpy array \n",
    "    returns: a 2D numpy array that contains the average of the constituent word vectors for each article\n",
    "    \"\"\"\n",
    "    num_features = model.syn0.shape[1]\n",
    "    articleFeatureVecs = np.zeros((len(articles),num_features),dtype=\"float32\")\n",
    "    counter = 0.\n",
    "    \n",
    "    # Loop through the articles\n",
    "    for article in articles:\n",
    "        #\n",
    "        # Print a status message every 1000th review\n",
    "        if counter%500. == 0.:\n",
    "            print (\"Article %d of %d\" % (counter, len(articles)))\n",
    "        # \n",
    "        # Call the function (defined above) that makes average feature vectors\n",
    "        articleFeatureVecs[counter] = makeFeatureVec(article, model)\n",
    "        #\n",
    "        # Increment the counter\n",
    "        counter = counter + 1.\n",
    "    return articleFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import articles for string cleaning\n",
    "\n",
    "def clean_text(text, stem_words = False, remove_stopwords = False):\n",
    "    ###clean\n",
    "    texts = re.sub(\"[^a-zA-Z]\",    #pattern to match\n",
    "              \" \",              #replace other with this\n",
    "              str(text))                 #text to apply to\n",
    "         \n",
    "    #print \"Text recived: \", texts \n",
    "    clean_corpus = texts.lower().split()\n",
    "    #print \"corpus: \", clean_corpus\n",
    "    if stem_words:\n",
    "        # Porter stemmer\n",
    "        porter = nltk.PorterStemmer()\n",
    "        # Snowball stemmer\n",
    "        snowball = nltk.SnowballStemmer('english')\n",
    "        # Lancaster stemmer\n",
    "        lancaster = nltk.LancasterStemmer()\n",
    "        # General stemming Lambda function to stem tokens\n",
    "        clean_corpus = lambda tokens: [porter.stem(w) for w in corpus]\n",
    "    if remove_stopwords:   # Optionally remove stop words\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        clean_corpus = [w for w in clean_corpus if not w in stops]\n",
    "        #print \"Clean_corpus: \", clean_corpus\n",
    "    \n",
    "    return (clean_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average feature vecs for articles\n",
      "Article 0 of 2498\n",
      "Article 500 of 2498\n",
      "Article 1000 of 2498\n",
      "Article 1500 of 2498\n",
      "Article 2000 of 2498\n"
     ]
    }
   ],
   "source": [
    "# ****************************************************************\n",
    "# Calculate average feature vectors for training and testing sets,\n",
    "# using the functions we defined above. We remove stopwords.\n",
    "\n",
    "print (\"Creating average feature vecs for articles\")\n",
    "cleaned_articles = []\n",
    "for article in inhibitor.TIABSMh:\n",
    "    cleaned_articles.append(clean_text(article, stem_words=False, remove_stopwords=True ))\n",
    "\n",
    "averageWordVecs = getAvgFeatureVecs(cleaned_articles, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1748\n",
      "750\n"
     ]
    }
   ],
   "source": [
    "RANDOM_STATE = 55\n",
    "TEST_SIZE = 0.3\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(averageWordVecs, inhibitor.Label, test_size=TEST_SIZE, \n",
    "                                                    random_state=RANDOM_STATE)\n",
    "\n",
    "print (len(X_train))\n",
    "print (len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0 1719]\n",
      " [   1   29]]\n",
      "[[  0 740]\n",
      " [  1  10]]\n"
     ]
    }
   ],
   "source": [
    "yTrFreq = scipy.stats.itemfreq(y_train)\n",
    "print(yTrFreq)\n",
    "\n",
    "yTrFreq = scipy.stats.itemfreq(y_test)\n",
    "print(yTrFreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight(x):\n",
    "    x = np.where(x==0, 1, 4)\n",
    "    return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting for Seed = 35\n",
      "('Support vectors: ', array([132,  17]))\n",
      "[[701  37]\n",
      " [  4   8]]\n",
      "Fitting for Seed = 71\n",
      "('Support vectors: ', array([81, 15]))\n",
      "[[717  17]\n",
      " [  9   7]]\n",
      "Fitting for Seed = 21\n",
      "('Support vectors: ', array([110,  15]))\n",
      "[[707  30]\n",
      " [  5   8]]\n",
      "Fitting for Seed = 61\n",
      "('Support vectors: ', array([133,  18]))\n",
      "[[709  26]\n",
      " [  4  11]]\n",
      "Fitting for Seed = 55\n",
      "('Support vectors: ', array([138,  19]))\n",
      "[[702  38]\n",
      " [  0  10]]\n",
      "SVM Cross validation results: \n",
      "Mean Precision: 0.24 +/- 0.05 \n",
      "  Mean  Recall: 0.69 +/- 0.18\n",
      "      Mean  F1: 0.34 +/- 0.05\n",
      "Mean  Accuracy: 0.95 +/- 0.01\n",
      "(' True Negative: ', [701, 717, 707, 709, 702])\n",
      "('False Negative: ', [4, 9, 5, 4, 0])\n",
      "(' True Positive: ', [8, 7, 8, 11, 10])\n",
      "('False Positive: ', [37, 17, 30, 26, 38])\n",
      "('Negative support Vectors: ', array([ 132.,   81.,  110.,  133.,  138.]))\n",
      "('Positive support Vectors: ', array([ 17.,  15.,  15.,  18.,  19.]))\n",
      "Mean positive support vectors: 16.80 +/- 1.60\n",
      "Mean Negative support vectors: 118.80 +/- 21.22\n",
      "Train positive: 25.80\n",
      "Train negattive: 1722.20\n",
      "Test positive: 13.20\n",
      "Test negative: 736.80\n"
     ]
    }
   ],
   "source": [
    "seeds = [35, 71, 21, 61, 55]\n",
    "\n",
    "svm_precision = []\n",
    "svm_recall = []\n",
    "svm_f = []\n",
    "acc = []\n",
    "ps_vectors = []\n",
    "ns_vectors = []\n",
    "tr_pos = []\n",
    "tr_neg = []\n",
    "te_pos = []\n",
    "te_neg = []\n",
    "true_pos = []\n",
    "true_neg = []\n",
    "false_pos = []\n",
    "false_neg = []\n",
    "\n",
    "for seed in seeds:\n",
    "    clf = SVC(C=10000.0, cache_size=200, class_weight='balanced', coef0=0.0,decision_function_shape=None, degree=3, \n",
    "          gamma='auto', kernel='linear', max_iter=-1, probability=False, random_state=seed, shrinking=True,\n",
    "          tol=0.001, verbose=False)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(averageWordVecs, inhibitor.Label, test_size=TEST_SIZE, \n",
    "                                                    random_state=seed)\n",
    "\n",
    "\n",
    "    w = weight(y_train)\n",
    "\n",
    "    yTrFreq = scipy.stats.itemfreq(y_train)\n",
    "    tr_pos = np.append(tr_pos, yTrFreq[1][1])\n",
    "    tr_neg = np.append(tr_neg, yTrFreq[0][1])\n",
    "\n",
    "    # print(yTrFreq)\n",
    "\n",
    "    yTrFreq = scipy.stats.itemfreq(y_test)\n",
    "    te_pos = np.append(te_pos, yTrFreq[1][1])\n",
    "    te_neg = np.append(te_neg, yTrFreq[0][1])\n",
    "\n",
    "    #print(yTrFreq)\n",
    "\n",
    "    print (\"Fitting for Seed = %d\" % seed)  \n",
    "\n",
    "    clf.fit(X_train, y_train, sample_weight = w)\n",
    "\n",
    "    pred = clf.predict(X_test)\n",
    "\n",
    "    s_vector = clf.n_support_\n",
    "    print (\"Support vectors: \", s_vector)            \n",
    "\n",
    "    s_prec = metrics.precision_score(y_test, pred)#, sample_weight=wt)\n",
    "    s_recall = metrics.recall_score(y_test, pred)#, sample_weight=wt)\n",
    "    s_f1 = metrics.f1_score(y_test, pred)#, sample_weight=wt)\n",
    "    \n",
    "    print (metrics.confusion_matrix(y_test, pred))\n",
    "    true_neg.append(metrics.confusion_matrix(y_test, pred)[0,0])\n",
    "    true_pos.append(metrics.confusion_matrix(y_test, pred)[1,1])\n",
    "    false_neg.append(metrics.confusion_matrix(y_test, pred)[1, 0])\n",
    "    false_pos.append(metrics.confusion_matrix(y_test, pred)[0, 1])\n",
    "\n",
    "    acc_s = metrics.accuracy_score(y_test, pred)\n",
    "    svm_precision.append(s_prec)\n",
    "    svm_recall.append(s_recall)\n",
    "    svm_f.append(s_f1)\n",
    "    acc.append(acc_s)\n",
    "    ps_vectors = np.append(ps_vectors, s_vector[1])\n",
    "    ns_vectors = np.append(ns_vectors, s_vector[0])      \n",
    "\n",
    "print (\"SVM Cross validation results: \")\n",
    "print (\"Mean Precision: %.2f +/- %.2f \"%(np.mean(svm_precision), np.std(svm_precision)))\n",
    "print (\"  Mean  Recall: %.2f +/- %.2f\" %(np.mean(svm_recall), np.std(svm_recall)))\n",
    "print (\"      Mean  F1: %.2f +/- %.2f\" %(np.mean(svm_f), np.std(svm_f)))\n",
    "print (\"Mean  Accuracy: %.2f +/- %.2f\" %(np.mean(acc), np.std(acc)))\n",
    "print (\" True Negative: \", true_neg)\n",
    "print (\"False Negative: \", false_neg)\n",
    "print (\" True Positive: \", true_pos)\n",
    "print (\"False Positive: \", false_pos)\n",
    "print (\"Negative support Vectors: \", ns_vectors)\n",
    "print (\"Positive support Vectors: \", ps_vectors)\n",
    "print (\"Mean positive support vectors: %.2f +/- %.2f\"%(np.mean(ps_vectors), np.std(ps_vectors)))\n",
    "print (\"Mean Negative support vectors: %.2f +/- %.2f\"%(np.mean(ns_vectors), np.std(ns_vectors)))\n",
    "print (\"Train positive: %.2f\" %(np.mean(tr_pos)))\n",
    "print (\"Train negattive: %.2f\" %(np.mean(tr_neg)))\n",
    "print (\"Test positive: %.2f\" %(np.mean(te_pos)))\n",
    "print (\"Test negative: %.2f\" %(np.mean(te_neg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
